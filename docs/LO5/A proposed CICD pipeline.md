*This pipeline focuses on the original 3 component (extension/wiki/database) design. This means there may be discussion on requirements and tests that do not exist in the rest of the document, due to this LO's uniquely forward looking, hypothetical nature. A future CI/CD pipeline would be more beneficial for a much more interlinked system.*

The unit tests for the database - the tests that ensure all the CRUD operations are working - would be automated after every single commit. These were previously identified as the most critical requirement in the test planning document, and due to the nature of unit tests, they are the easiest to automate. Complimentary, using TDD in the development process has meant the additional scaffolding for this integration into CI/CD is minimal as the tests are already well established in the suite. These tests fit nicely into a CI/CD pipeline; they are easily added to and I cannot foresee any issues involving these tests.

The performance tests for the database will likely have to be ran at certain intervals, or delayed until such. These are critical to ensure a change (that may have passed all other unit/integration/system tests) is not negatively impacting performance, which is a key requirement for the software if it ever intends to pick up a large active userbase. However, the most likely outcome for integrating this into CI/CD will be running it on the actual server hardware. By design, the database will have periodic publicly downloadable backups available, and so as long as we time the performance testing to after these, we can:
- revert to a backup in case the data gets corrupted during testing
- give users a usable "copy" of the data
The second point is important as we'd need to inform all active users of the ongoing testing. It would also be advisable to entirely cut off access (e.g. *maintenance mode*) for users: even if we were to explain the server would be under above average load, the average customer may still be annoyed at this.

Performance tests for the extension will have to be routinely ran. The extension is the toughest element of this to maintain as it is dependant on the structure of *YouTube's* pages, which are likely going through a CI/CD cycle themselves at *Google*. This means all tests for the extension will have to be reran constantly to keep up with the constant (*and often undocumented/unannounced*) changes to *YouTube*.

The unit tests for the extension are also mostly automatable. They involve simulating user input to the extension and ensuring the correct outputs (either database queries or HTML based on stored values). The only issue with full automation may come with changes to the HTML; even by attempting future-proofing (e.g. grabbing HTML elements by their ID), it is possible that redesigns (based on user feedback post-release, for example) will change which elements have the actual functionality. In a best case scenario, these tests should be checked to see *why* they failed, and changed accordingly if this edge case is met.

Integration tests (for the extension-database) follow a similar model. Any major changes made to interactions must involve the tests being updated. This will likely happen (as mentioned above) in the defect detection stage of the CI/CD development loop, where we must investigate if the core code or tests themselves are what caused the suite to fail.
For integration tests involving the wiki, this is a greater risk. The structure of the wiki (possibly with knock-on effects to the database) will be moulded by the community surrounding it and interactions may be changed due to community votes. With every major revision to guidelines/rules/standards, the integration tests on all 3 components must be re-ran.